# Code Structure Documentation

This document explains the structure and organization of the analysis code.

---

## ğŸ“‚ Repository Organization

```
ORGANIZED_PAPER_CODE/
â”‚
â”œâ”€â”€ 01_Final_Analysis_Code/
â”‚   â”œâ”€â”€ Final_Paper_Analysis_10Tables_5Figures.ipynb    # â­ Main file - USE THIS
â”‚   â”œâ”€â”€ BACKUP_Original_fincode_formerged_2023-12-07.ipynb
â”‚   â”œâ”€â”€ BACKUP_Development_v1_2023-11-09.ipynb
â”‚   â””â”€â”€ BACKUP_Development_v2_2023-11-09.ipynb
â”‚
â”œâ”€â”€ 02_Paper_Results/
â”‚   â”œâ”€â”€ figures/          # 5 figure files (docx)
â”‚   â”œâ”€â”€ tables/           # 10 table files (docx)
â”‚   â””â”€â”€ output_data/      # CSV result files
â”‚
â”œâ”€â”€ 03_Source_Data/
â”‚   â”œâ”€â”€ ì½”ìŠ¤í”¼ ìê¸°ì£¼ì‹ì·¨ë“ë°ì²˜ë¶„.xlsx    # KOSPI raw data
â”‚   â”œâ”€â”€ ì½”ìŠ¤ë‹¥ ìê¸°ì£¼ì‹ì·¨ë“ë°ì²˜ë¶„.xlsx    # KOSDAQ raw data
â”‚   â””â”€â”€ final_data_summary.csv
â”‚
â””â”€â”€ 04_Documentation/
    â”œâ”€â”€ README.md                      # Overview
    â”œâ”€â”€ Code_Structure.md             # This file
    â””â”€â”€ Paper_Output_Mapping.md       # Detailed mapping
```

---

## ğŸ¯ Main Analysis File

### Final_Paper_Analysis_10Tables_5Figures.ipynb

**Structure**: 52 cells (41 code + 11 markdown headers)

#### Cell Organization:

```
Section 1: Data Loading
â”œâ”€â”€ Cell 0: Load KOSPI/KOSDAQ data
â”‚
Section 2: Descriptive Statistics (Table 1)
â”œâ”€â”€ Cells 2-4: Data exploration
â””â”€â”€ Cell 5: Generate Table 1
â”‚
Section 3: Data Preprocessing
â”œâ”€â”€ Cells 9-10: Remove missing/unknown values
â”œâ”€â”€ Cell 11: One-hot encoding
â””â”€â”€ Cells 12-13: Verify cleaning
â”‚
Section 4: Behavior Analysis (Table 2)
â””â”€â”€ Cell 16: Group by behavior, calculate Tobin Q â†’ Table 2
â”‚
Section 5: Feature Selection & Scaling
â”œâ”€â”€ Cells 18-19: Select 6 features
â””â”€â”€ Cells 20-21: StandardScaler
â”‚
Section 6: K-Means Clustering (Figures 1-2, Table 3)
â”œâ”€â”€ Cell 22: Elbow plot â†’ Figure 1
â”œâ”€â”€ Cell 23: Silhouette score â†’ Figure 2
â”œâ”€â”€ Cells 24-27: Fit KMeans(k=3)
â””â”€â”€ Cell 28: Cluster means â†’ Table 3
â”‚
Section 7: Cluster-Behavior Analysis (Table 4, Figure 3)
â””â”€â”€ Cell 33: Cross-tabulation â†’ Table 4, Figure 3
â”‚
Section 8: ML Models (Table 5, Figure 5)
â”œâ”€â”€ Cells 38, 42, 43: Prepare data, split train/test
â”œâ”€â”€ Cells 47-48: Train 4 models
â””â”€â”€ Cell 49: Evaluate â†’ Table 5, Figure 5
â”‚
Section 9: Feature Importance (Tables 6-8, Figure 4)
â”œâ”€â”€ Cells 57-58: Train by behavior
â”œâ”€â”€ Cell 59: FI for Burned â†’ Table 6
â”œâ”€â”€ Cell 60: FI for Disposed_Slower â†’ Table 7
â”œâ”€â”€ Cell 61: FI for Long-term Holding â†’ Table 8
â””â”€â”€ Cell 62: Visualize â†’ Figure 4
â”‚
Section 10: Value Judgment (Table 10)
â”œâ”€â”€ Cells 99, 106, 108: Define criteria, train models
â””â”€â”€ Cell 128: Frequencies â†’ Table 10
â”‚
Section 11: Overall Feature Importance (Table 9)
â””â”€â”€ Cells 135-136: Aggregate and rank â†’ Table 9
```

---

## ğŸ”„ Data Flow

### 1. Data Loading & Cleaning
```
Raw Excel files
    â†“
Load & merge (Cell 0)
    â†“
Remove missing values (Cell 9)
    â†“
Remove 'Unknown' behaviors (Cell 10)
    â†“
One-hot encoding (Cell 11)
    â†“
Clean dataset
```

### 2. Feature Engineering
```
Clean dataset
    â†“
Select 6 features (Cell 18)
    â†“
StandardScaler (Cell 20)
    â†“
Scaled feature matrix X
```

### 3. Clustering Analysis
```
Scaled features X
    â†“
Elbow method (Cell 22) â†’ Figure 1
    â†“
Silhouette score (Cell 23) â†’ Figure 2
    â†“
KMeans(k=3) (Cell 24)
    â†“
Cluster labels
    â†“
Cluster means (Cell 28) â†’ Table 3
```

### 4. ML Pipeline
```
Scaled features X + Cluster labels y
    â†“
Create binary labels per cluster (Cell 42)
    â†“
Train/Test split 70/30 (Cell 43)
    â†“
Train 4 models (Cell 48):
    - AdaBoost
    - XGBoost
    - Gradient Boosting
    - Random Forest
    â†“
Evaluate accuracy (Cell 49) â†’ Table 5, Figure 5
    â†“
Extract feature importance (Cells 57-62) â†’ Tables 6-8, Figure 4
```

### 5. Value Judgment
```
Features + Behavior labels
    â†“
Define value criteria (Cell 99)
    â†“
Create judgment labels (Cell 106)
    â†“
Train classification (Cell 108)
    â†“
Frequency distribution (Cell 128) â†’ Table 10
```

---

## ğŸ§© Code Dependencies

### Cell Dependencies:

```
Cell 0 (Load data)
    â†“
Cells 2-5 (Table 1) â† Independent, can run early
    â†“
Cells 9-13 (Preprocessing) â† Required for all downstream
    â†“
Cell 16 (Table 2) â† Can run after preprocessing
    â†“
Cells 18-21 (Feature scaling) â† Required for clustering & ML
    â†“
Cells 22-28 (Clustering) â† Generates cluster labels
    â†“  â†“
    â†“  Cell 33 (Table 4, Figure 3) â† Needs cluster labels
    â†“
Cells 38-49 (ML models) â† Needs cluster labels & scaled features
    â†“
Cells 57-62 (Feature importance) â† Needs ML models trained
    â†“
Cells 99-128 (Value judgment) â† Can run in parallel with FI
    â†“
Cells 135-136 (Table 9) â† Aggregates all FI results
```

### Key Dependencies:
- **All sections** depend on: Cell 0 (data loading)
- **Clustering & ML** depend on: Cells 9-13 (preprocessing) + Cells 18-21 (scaling)
- **Feature importance** depends on: Clustering results (cluster labels)
- **Table 9** depends on: All feature importance analysis (Cells 57-62)

---

## ğŸ“¦ Key Variables

### Data Variables:
```python
# Main dataset (after Cell 0)
df: pd.DataFrame  # Merged KOSPI + KOSDAQ data

# After preprocessing (Cell 13)
df_clean: pd.DataFrame  # Cleaned dataset

# Feature matrix (Cell 19)
X: np.ndarray  # Shape: (n_samples, 6)

# Scaled features (Cell 20)
X_scaled: np.ndarray  # StandardScaler applied
```

### Target Variables:
```python
# Cluster labels (Cell 25)
cluster_labels: np.ndarray  # Values: 0, 1, 2

# Behavior types (Cell 16)
behavior: pd.Series  # Values: 'Burned', 'Disposed_Slower', 'Long-term Holding'

# Value judgment (Cell 106)
value_judgment: pd.Series  # Values: 0 (undervalued), 1 (fairly valued)
```

### Model Variables:
```python
# ML models (Cell 47)
models = {
    'AdaBoost': AdaBoostClassifier(random_state=0),
    'XGBoost': XGBClassifier(random_state=0),
    'GradientBoosting': GradientBoostingClassifier(random_state=0),
    'RandomForest': RandomForestClassifier(random_state=0)
}
```

---

## ğŸ› ï¸ Key Functions & Methods

### Data Processing:
```python
# Load data
pd.read_excel()
pd.concat()

# Cleaning
df.dropna()
df[df['Behavior'] != 'Unknown']
pd.get_dummies()

# Feature selection
df[['ìì‚°ì´ê³„', 'ë¶€ì±„ì´ê³„', ...]]
```

### Clustering:
```python
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

kmeans = KMeans(n_clusters=3, random_state=0)
cluster_labels = kmeans.fit_predict(X_scaled)

score = silhouette_score(X_scaled, cluster_labels)
```

### Machine Learning:
```python
from sklearn.model_selection import train_test_split
from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, GradientBoostingClassifier
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score

X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.3, random_state=0, stratify=y
)

model.fit(X_train, y_train)
y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
```

### Feature Importance:
```python
# For tree-based models
feature_importance = model.feature_importances_

# Create importance dataframe
importance_df = pd.DataFrame({
    'feature': feature_names,
    'importance': feature_importance
}).sort_values('importance', ascending=False)
```

---

## ğŸ“Š Output Generation

### Tables (DataFrames):
```python
# Table 1: Descriptive statistics
df.describe()

# Table 2: Behavior analysis
df.groupby('Behavior')['Tobin_Q'].mean()

# Table 3: Cluster means
df.groupby('Cluster')[features].mean()

# Table 4: Cross-tabulation
pd.crosstab(df['Cluster'], df['Behavior'])

# Table 5: Model performance
pd.DataFrame({'Model': [...], 'Accuracy': [...]})

# Tables 6-8: Feature importance by behavior
importance_df.head(10)

# Table 9: Overall feature importance
all_importance_df.groupby('feature').mean().sort_values()

# Table 10: Value judgment frequencies
df['Value_Judgment'].value_counts()
```

### Figures (Matplotlib/Seaborn):
```python
import matplotlib.pyplot as plt
import seaborn as sns

# Figure 1: Elbow plot
plt.plot(k_values, inertias)
plt.xlabel('Number of clusters')
plt.ylabel('Inertia')

# Figure 2: Silhouette score
plt.bar(['k=3'], [silhouette_score])

# Figure 3: Cluster-Behavior distribution
sns.countplot(x='Cluster', hue='Behavior', data=df)

# Figure 4: Feature importance
plt.barh(features, importances)

# Figure 5: Model performance
plt.bar(model_names, accuracies)
```

---

## ğŸ”§ Configuration Parameters

### Global Settings:
```python
random_state = 0          # For reproducibility
test_size = 0.3           # 70/30 train/test split
stratify = True           # Stratified sampling
```

### Clustering:
```python
n_clusters = 3            # Optimal k
max_k = 10                # For elbow plot
```

### Feature Selection:
```python
n_features = 6            # Selected features
scaling_method = 'StandardScaler'
```

---

## ğŸ“ Code Style & Conventions

### Variable Naming:
- DataFrames: `df`, `df_clean`, `df_train`
- Arrays: `X`, `y`, `X_scaled`
- Models: `model`, `clf`, or descriptive names like `ada_boost`
- Results: `accuracy`, `importance`, `predictions`

### Comments:
- Section headers: Clear markdown cells
- Code comments: For complex operations only
- Output descriptions: Before each table/figure generation

### File Saving:
- CSV files: `df.to_csv('filename.csv', index=False)`
- Figures: Manual save from notebook (not automated in code)
- Tables: Manual copy to Word (not automated in code)

---

## âš¡ Performance Notes

### Execution Time:
- Data loading: ~5 seconds
- Preprocessing: ~2 seconds
- Clustering: ~10 seconds
- ML model training: ~30 seconds (all 4 models)
- Feature importance: ~20 seconds
- **Total**: ~2-3 minutes for full notebook execution

### Memory Usage:
- Raw data: ~50 MB
- Processed data: ~30 MB
- Models: ~10 MB
- **Total**: ~100 MB peak memory

---

## ğŸ› Common Issues & Solutions

### Issue 1: Data file not found
```python
# Solution: Check data path in Cell 0
# Adjust to: '../03_Source_Data/filename.xlsx'
```

### Issue 2: Korean encoding errors
```python
# Solution: Ensure UTF-8 encoding
pd.read_excel('file.xlsx', encoding='utf-8')
```

### Issue 3: Missing library
```bash
# Solution: Install required packages
pip install pandas numpy scikit-learn xgboost matplotlib seaborn
```

### Issue 4: Different results
```python
# Solution: Check random_state is set to 0 in all models
# Verify data cleaning steps are consistent
```

---

## ğŸ“š Additional Resources

### Related Files:
- `Paper_Output_Mapping.md` - Detailed cell-to-output mapping
- `README.md` - Project overview and quick start guide

### Python Documentation:
- scikit-learn: https://scikit-learn.org/
- XGBoost: https://xgboost.readthedocs.io/
- Pandas: https://pandas.pydata.org/

---

**Document Version**: 1.0
**Last Updated**: 2026-01-07
**Corresponds to**: Final_Paper_Analysis_10Tables_5Figures.ipynb (52 cells)
